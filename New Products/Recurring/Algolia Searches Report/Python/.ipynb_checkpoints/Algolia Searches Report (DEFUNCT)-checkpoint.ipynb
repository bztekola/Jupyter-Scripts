{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pillars meeting notes 02/19/2019\n",
    "* algolia report send every week  \n",
    "* add in changes over time  \n",
    "* maybe some benchmarks  \n",
    "* look around, explore, develop this, find some insights  \n",
    "* look at searches with zero hits  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Email Resources\n",
    "* Email with subject line \"Fwd: Discrepancy between Google Analytics and Algolia\"\n",
    "* 2018-07-21 is when zephyr updated the tracking capabilities of GA, see email mentioned above\n",
    "* Email with subject line \"How to view defined Event Actions in Google Analytics\"\n",
    "* Email with subject line \"Google Analytics: Shop vs www.Adafruit.com\"\n",
    "\n",
    "### Google Analytics resources\n",
    "* [Difference between Entrances and Sessions](https://support.google.com/analytics/answer/2956047?hl=en)\n",
    "* [Behavior Flow help page](https://support.google.com/analytics/answer/2785577?hl=en)\n",
    "* [Difference between Starting Page and Landing Page in Behavior Flow](https://webmasters.stackexchange.com/questions/99267/whats-the-difference-between-landing-page-and-starting-page-in-behaviour-fl)\n",
    "* [Info on PagePathLevels and Session count discrepancy](https://www.quora.com/In-Google-analytics-custom-Reporting-what-is-Page-path-level-definition)\n",
    "* [How Site Search metrics are calculated](https://support.google.com/analytics/answer/1032321?hl=en)\n",
    "\n",
    "### Dimensions and Metrics Explorer for API\n",
    "* https://developers.google.com/analytics/devguides/reporting/core/dimsmets\n",
    "\n",
    "### Google2Pandas Docs\n",
    "* https://github.com/panalysis/Google2Pandas\n",
    "\n",
    "### Other notes\n",
    "* Do this for Shop, Learn Guides, and Internal (for Tom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== jb_google_analytics start ==\n",
      "\n",
      "Your ga function is: get_ga(account_id, date_start, date_end, dimensions, metrics, filters)\n",
      "\n",
      "Beware of sampling!\n",
      "\n",
      "== jb_google_analytics end ==\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'/Users/jarad/Fake Folder/Python Libraries/')\n",
    "\n",
    "from jb_libraries import *\n",
    "from jb_google_analytics import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_end = '2019-03-03' # should be the most recent sunday, if you run this every monday\n",
    "\n",
    "one_year_ago = str((pd.to_datetime(date_end) - pd.DateOffset(weeks = 52, days = -1)).date())\n",
    "three_months_ago = str((pd.to_datetime(date_end) - pd.DateOffset(weeks = 12, days = 6)).date())\n",
    "four_weeks_ago = str((pd.to_datetime(date_end) - pd.DateOffset(weeks = 4, days = -1)).date())\n",
    "one_week_ago = str((pd.to_datetime(date_end) - pd.DateOffset(weeks = 1, days = -1)).date())\n",
    "\n",
    "write_workbook = 'yes'\n",
    "\n",
    "# 'year and month','year and quarter','week ending'\n",
    "report_type = 'week ending'\n",
    "\n",
    "csv_path = '/Users/jarad/Fake Folder/New Products/Recurring/Algolia Searches Report/CSVs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w.e 2019-03-03 - Algolia Searches Report\n"
     ]
    }
   ],
   "source": [
    "t = ' - Algolia Searches Report'\n",
    "\n",
    "d = pd.to_datetime(date_end)\n",
    "\n",
    "year = str(d.year)\n",
    "quarter = str(d.quarter)\n",
    "month_num = date_end[5:7]\n",
    "month_name = calendar.month_abbr[int(month_num)]\n",
    "month_num = str(month_num)\n",
    "\n",
    "if report_type == 'year and quarter':\n",
    "    workbook_title = year + ' - ' + 'Q' + quarter + t\n",
    "elif report_type == 'year and month':\n",
    "    workbook_title = year + ' - ' + month_num + ' - ' + month_name + t\n",
    "elif report_type == 'week ending':\n",
    "    workbook_title = 'w.e ' + date_end + t\n",
    "else:\n",
    "    workbook_title = 'no title'\n",
    "    \n",
    "print(workbook_title)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "accounts = {97515018:'shop',\n",
    "            65659776:'learn',\n",
    "            15556579:'www.adafruit.com'}\n",
    "'''\n",
    "\n",
    "adafruit_id = 15556579"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define page_clean function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_guides = pd.read_sql(\n",
    "'''\n",
    "SELECT\n",
    "LOWER(url) AS url\n",
    "FROM learn_guides\n",
    "''', db)\n",
    "\n",
    "col_fix(learn_guides)\n",
    "\n",
    "learn_guides['url clean'] = learn_guides['url'].str.replace('https://learn.adafruit.com','')\n",
    "\n",
    "# list of tuples vs dictionary, of regex:value pairs\n",
    "# https://stackoverflow.com/questions/33343680/can-a-regular-expression-be-used-as-a-key-in-a-dictionary\n",
    "\n",
    "def page_clean(x):\n",
    "    \n",
    "    d = [\n",
    "         ('category|categories','categories'),\n",
    "         ('search','search'),\n",
    "         ('^\\/\\?q=','search'),\n",
    "         ('^\\/$','search'),\n",
    "         ('checkout','checkout'),\n",
    "         ('^\\/product','product'),\n",
    "         ('wishlist','wishlists'),\n",
    "         ('account_history_info','account history info'),\n",
    "         ('galleries','galleries'),\n",
    "         ('back_in_stock_notification_subscribe','back in stock subscribe'),\n",
    "         ('manage_subscription','manage subscription')\n",
    "        ]\n",
    "        \n",
    "    # find learn guides    \n",
    "    if any(l in x for l in learn_guides['url clean'].tolist()) or re.search('learn|asset|guide', x):\n",
    "        return 'learn guide'\n",
    "    \n",
    "    # if the page title is \"/some_number\"\n",
    "    elif re.search('^\\/[0-9]*/*$', x):\n",
    "        return 'product'\n",
    "    \n",
    "    # if the page title is \"/some_word\" or \"/some_word/\" and not just \"/\" or \"/some_number\"\n",
    "    # take that single word\n",
    "    elif re.search(r'^/[a-z0-9_]*/*$', x) and re.search('^\\/$', x) == None and re.search('^\\/[0-9]*/*$', x) == None:\n",
    "        return x.replace('/','')\n",
    "    \n",
    "    # go through regex dict\n",
    "    else:\n",
    "        for k,v in d:\n",
    "            if re.search(k, x):\n",
    "                return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define get_trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trend(array):\n",
    "    \n",
    "    x = range(len(array))\n",
    "    y = array\n",
    "    \n",
    "    z = np.polyfit(x,y,1)\n",
    "    slope = z[0]\n",
    "    intercept = z[1]\n",
    "    \n",
    "    return slope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*****\n",
    "# Check out all GA hostnames\n",
    "*****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "hostnames = get_ga(15556579, # www.adafruit.com account ID\n",
    "                  one_week_ago,\n",
    "                  date_end,\n",
    "                  ['hostname'],\n",
    "                  ['pageviews'],\n",
    "                  None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a208cfa90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "g1 = hostnames.groupby('hostname')[['pageviews']].sum()\n",
    "g1.sort_values('pageviews', ascending = False, inplace = True)\n",
    "\n",
    "g1['% of total'] = g1['pageviews']/g1['pageviews'].sum()\n",
    "g1['% running sum'] = g1['% of total'].cumsum()\n",
    "\n",
    "q = 0.90\n",
    "g2 = g1[g1['% running sum'] <= q]\n",
    "\n",
    "g2['pageviews'].plot(figsize = (10,5),\n",
    "                     kind = 'bar', \n",
    "                     color = 'purple',\n",
    "                     edgecolor = 'black',\n",
    "                     alpha = 0.75,\n",
    "                     lw = 3,\n",
    "                     rot = 45,\n",
    "                    title = 'top {:,.0f}% of hostnames by pageviews'.format(q*100))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*****\n",
    "# Get overall GA stats\n",
    "*****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = dt.datetime.now()\n",
    "\n",
    "overall_super_main = pd.DataFrame()\n",
    "    \n",
    "df = get_ga(k,\n",
    "            three_months_ago,\n",
    "            date_end,\n",
    "\n",
    "            ['date'], \n",
    "\n",
    "            ['sessions',\n",
    "             'searchSessions',\n",
    "             'users',\n",
    "             'searchUniques', # Total number of unique keywords from internal searches within a session. For example, if \"shoes\" was searched for 3 times in a session, it would be counted only once.\n",
    "             'searchExits', # The number of exits on the site that occurred following a search result from the site's internal search feature.\n",
    "             'pageviews'],\n",
    "\n",
    "            None)\n",
    "       \n",
    "overall_super_main['account name'] = overall_super_main['account id'].map(accounts)    \n",
    "overall_super_main.rename(columns = {'search sessions':'sessions with search',\n",
    "                                     'search uniques':'unique searches'}, inplace = True)\n",
    "\n",
    "# get week ending, using my function\n",
    "df = jb_week_ending(overall_super_main, 'date', 'Sunday')\n",
    "overall_super_main = pd.merge(overall_super_main, df, on = 'date')\n",
    "\n",
    "e = dt.datetime.now()\n",
    "print(e-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:02:13.416374\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "s = dt.datetime.now()\n",
    "\n",
    "overall_super_main = pd.DataFrame()\n",
    "\n",
    "for k in accounts.keys():\n",
    "    \n",
    "    df = get_ga(k,\n",
    "                three_months_ago,\n",
    "                date_end,\n",
    "                \n",
    "                ['date'], \n",
    "                \n",
    "                ['sessions',\n",
    "                 'searchSessions',\n",
    "                 'users',\n",
    "                 'searchUniques', # Total number of unique keywords from internal searches within a session. For example, if \"shoes\" was searched for 3 times in a session, it would be counted only once.\n",
    "                 'searchExits', # The number of exits on the site that occurred following a search result from the site's internal search feature.\n",
    "                 'pageviews'],\n",
    "               \n",
    "                None)\n",
    "    \n",
    "    overall_super_main = pd.concat([overall_super_main, df], sort = False, ignore_index = True)\n",
    "    \n",
    "overall_super_main['account name'] = overall_super_main['account id'].map(accounts)    \n",
    "overall_super_main.rename(columns = {'search sessions':'sessions with search',\n",
    "                                     'search uniques':'unique searches'}, inplace = True)\n",
    "\n",
    "# get week ending, using my function\n",
    "df = jb_week_ending(overall_super_main, 'date', 'Sunday')\n",
    "overall_super_main = pd.merge(overall_super_main, df, on = 'date')\n",
    "\n",
    "e = dt.datetime.now()\n",
    "print(e-s)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_main = overall_super_main.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "d1 = overall_main['date'].min().date()\n",
    "d2 = overall_main['date'].max().date()\n",
    "days = (pd.to_datetime(d2) - pd.to_datetime(d1)).days\n",
    "\n",
    "for account_name in set(overall_main['account name']):\n",
    "    \n",
    "    report_dict[account_name + ' overall period'] = '%s to %s (%i days)' % (d1,d2,days)\n",
    "    \n",
    "#=========================\n",
    "# get data by report_type\n",
    "#=========================\n",
    "    \n",
    "    a1 = overall_main[overall_main['account name'] == account_name]\n",
    "    a2 = a1.groupby(report_type)[['users',\n",
    "                                  'sessions',\n",
    "                                  'sessions with search',\n",
    "                                  'unique searches',\n",
    "                                  'search exits',\n",
    "                                  'pageviews']].sum()\n",
    "\n",
    "    a2['% of sessions with search'] = a2['sessions with search']/a2['sessions']\n",
    "    a2['avg search per search session'] = a2['unique searches']/a2['sessions with search']\n",
    "    a2['avg pageview per session'] = a2['pageviews']/a2['sessions']\n",
    "    a2['search exits as % of unique searches'] = a2['search exits']/a2['unique searches']\n",
    "    a2.fillna(0, inplace = True)\n",
    "    \n",
    "    old_cols = a2.columns.tolist()\n",
    "\n",
    "    cols1 = sorted([x for x in old_cols if 'search' in x], reverse = True)\n",
    "    cols2 = sorted([x for x in old_cols if 'pageview' in x], reverse = True)\n",
    "    cols3 = [x for x in old_cols if x not in cols1 + cols2]\n",
    "\n",
    "    new_cols = cols3 + cols1 + cols2\n",
    "    a2 = a2[new_cols]\n",
    "    \n",
    "    report_dict[account_name + ' overall over time'] = a2\n",
    "    \n",
    "#=========================\n",
    "# get confidence interval\n",
    "#=========================    \n",
    "    \n",
    "    conf = jb_conf(a2).loc[:'upper'].T\n",
    "    \n",
    "    report_dict[account_name + ' overall conf'] = conf\n",
    "    \n",
    "#=========================\n",
    "# get trend\n",
    "#=========================    \n",
    "    \n",
    "    a3 = a2.T\n",
    "    vals = a3.values\n",
    "    ls = []\n",
    "    for i in range(len(vals)):\n",
    "        slope = get_trend(vals[i])\n",
    "        results = 'up' if slope > 0 else 'down'\n",
    "        ls.append(results)    \n",
    "        \n",
    "#=========================\n",
    "# isolate most recent week\n",
    "#=========================    \n",
    "\n",
    "    a4 = pd.DataFrame(a3.iloc[:,-1])\n",
    "    a4.columns = ['week ending ' + x for x in a4.columns]\n",
    "    \n",
    "#=========================\n",
    "# add in avg and trend\n",
    "#=========================    \n",
    "\n",
    "    a4['average'] = conf['mean']\n",
    "    a4['week to week trend'] = ls\n",
    "    \n",
    "#=========================\n",
    "# add in confidence interval results\n",
    "#=========================    \n",
    "\n",
    "    ls = []\n",
    "    for index, row in a4.iterrows():\n",
    "        lower, mean, upper = conf.loc[index].values\n",
    "        val = row.values[0]\n",
    "        if np.logical_and(val >= lower, val <= upper):\n",
    "            res = 'normal'\n",
    "        else:\n",
    "            if val < lower:\n",
    "                res = 'BELOW normal'\n",
    "            else:\n",
    "                res = 'ABOVE normal'\n",
    "                \n",
    "        ls.append(res)\n",
    "        \n",
    "    a4['comment on current value'] = ls\n",
    "    \n",
    "#=========================\n",
    "# add to dict\n",
    "#=========================        \n",
    "       \n",
    "    report_dict[account_name + ' overall current'] = a4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "www.adafruit.com data is from 2018-12-03 to 2019-03-03 (90 days)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>week ending 2019-03-03</th>\n",
       "      <th>average</th>\n",
       "      <th>week to week trend</th>\n",
       "      <th>comment on current value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>users</th>\n",
       "      <td>434,029</td>\n",
       "      <td>4.288528e+05</td>\n",
       "      <td>up</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sessions</th>\n",
       "      <td>562,961</td>\n",
       "      <td>5.511402e+05</td>\n",
       "      <td>up</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique searches</th>\n",
       "      <td>85,875</td>\n",
       "      <td>8.240754e+04</td>\n",
       "      <td>up</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sessions with search</th>\n",
       "      <td>35,490</td>\n",
       "      <td>3.350131e+04</td>\n",
       "      <td>up</td>\n",
       "      <td>ABOVE normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>search exits as % of unique searches</th>\n",
       "      <td>11.9%</td>\n",
       "      <td>1.228415e-01</td>\n",
       "      <td>down</td>\n",
       "      <td>BELOW normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>search exits</th>\n",
       "      <td>10,199</td>\n",
       "      <td>1.010200e+04</td>\n",
       "      <td>up</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg search per search session</th>\n",
       "      <td>2.42</td>\n",
       "      <td>2.460705e+00</td>\n",
       "      <td>down</td>\n",
       "      <td>BELOW normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>% of sessions with search</th>\n",
       "      <td>6.3%</td>\n",
       "      <td>6.074653e-02</td>\n",
       "      <td>up</td>\n",
       "      <td>ABOVE normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pageviews</th>\n",
       "      <td>1,559,193</td>\n",
       "      <td>1.520443e+06</td>\n",
       "      <td>up</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg pageview per session</th>\n",
       "      <td>3</td>\n",
       "      <td>2.760851e+00</td>\n",
       "      <td>down</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     week ending 2019-03-03       average  \\\n",
       "users                                               434,029  4.288528e+05   \n",
       "sessions                                            562,961  5.511402e+05   \n",
       "unique searches                                      85,875  8.240754e+04   \n",
       "sessions with search                                 35,490  3.350131e+04   \n",
       "search exits as % of unique searches                  11.9%  1.228415e-01   \n",
       "search exits                                         10,199  1.010200e+04   \n",
       "avg search per search session                          2.42  2.460705e+00   \n",
       "% of sessions with search                              6.3%  6.074653e-02   \n",
       "pageviews                                         1,559,193  1.520443e+06   \n",
       "avg pageview per session                                  3  2.760851e+00   \n",
       "\n",
       "                                     week to week trend  \\\n",
       "users                                                up   \n",
       "sessions                                             up   \n",
       "unique searches                                      up   \n",
       "sessions with search                                 up   \n",
       "search exits as % of unique searches               down   \n",
       "search exits                                         up   \n",
       "avg search per search session                      down   \n",
       "% of sessions with search                            up   \n",
       "pageviews                                            up   \n",
       "avg pageview per session                           down   \n",
       "\n",
       "                                     comment on current value  \n",
       "users                                                  normal  \n",
       "sessions                                               normal  \n",
       "unique searches                                        normal  \n",
       "sessions with search                             ABOVE normal  \n",
       "search exits as % of unique searches             BELOW normal  \n",
       "search exits                                           normal  \n",
       "avg search per search session                    BELOW normal  \n",
       "% of sessions with search                        ABOVE normal  \n",
       "pageviews                                              normal  \n",
       "avg pageview per session                               normal  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 'www.adafruit.com'\n",
    "\n",
    "print('%s data is from %s' % (a,report_dict[a + ' overall period']))\n",
    "\n",
    "fmt = ['{:,.0f}','{:,.0f}','{:,.0f}','{:,.0f}','{:,.1f}%','{:,.0f}','{:,.2f}','{:,.1f}%','{:,.0f}','{:,.0f}']\n",
    "\n",
    "mult = []\n",
    "for f in fmt:\n",
    "    if '%' in f:\n",
    "        mult.append(100)\n",
    "    else:\n",
    "        mult.append(1)\n",
    "\n",
    "vals = list(report_dict[a + ' overall current'].iloc[:,:1].values.flatten())\n",
    "\n",
    "ls = []\n",
    "for i in range(len(vals)):\n",
    "    f = fmt[i].format(vals[i] * mult[i])\n",
    "    ls.append(f)\n",
    "    \n",
    "df = report_dict[a + ' overall current'].copy()\n",
    "df.iloc[:,:1] = ls\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*****\n",
    "# Get checkout data\n",
    "*****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:02:29.031696\n"
     ]
    }
   ],
   "source": [
    "s = dt.datetime.now()\n",
    "\n",
    "checkout_super_main = pd.DataFrame()\n",
    "\n",
    "for k in accounts.keys():\n",
    "    \n",
    "    df = get_ga(k,\n",
    "                three_months_ago,\n",
    "                date_end,\n",
    "                \n",
    "                ['date',\n",
    "                'eventAction'], \n",
    "                \n",
    "                ['totalEvents'],\n",
    "                \n",
    "                ['eventCategory==Ecommerce'])\n",
    "    \n",
    "    checkout_super_main = pd.concat([checkout_super_main, df], sort = False, ignore_index = True)\n",
    "    \n",
    "checkout_super_main['account name'] = checkout_super_main['account id'].map(accounts)    \n",
    "checkout_super_main.rename(columns = {'search sessions':'sessions with search',\n",
    "                                     'search uniques':'unique searches'}, inplace = True)\n",
    "\n",
    "# get week ending, using my function\n",
    "df = jb_week_ending(checkout_super_main, 'date', 'Sunday')\n",
    "checkout_super_main = pd.merge(checkout_super_main, df, on = 'date')\n",
    "\n",
    "e = dt.datetime.now()\n",
    "print(e-s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkout_main = checkout_super_main.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list of event actions\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['add impression',\n",
       " 'add to cart',\n",
       " 'add+impression',\n",
       " 'checkout',\n",
       " 'ghostery',\n",
       " 'purchase',\n",
       " 'view product']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('list of event actions')\n",
    "sorted(list(set(checkout_main['event action'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = checkout_main['date'].min().date()\n",
    "d2 = checkout_main['date'].max().date()\n",
    "days = (pd.to_datetime(d2) - pd.to_datetime(d1)).days\n",
    "\n",
    "event_actions = ['add to cart','checkout','purchase']\n",
    "\n",
    "for account_name in set(checkout_main['account name']):\n",
    "    \n",
    "    report_dict[account_name + ' checkouts period'] = '%s to %s (%i days)' % (d1,d2,days)\n",
    "    \n",
    "    a1 = checkout_main[checkout_main['event action'].isin(event_actions)].groupby([report_type,'event action'])[['total events']].sum().unstack()\n",
    "    a1.columns = a1.columns.droplevel(0)\n",
    "    a1.sort_values(a1.index[-1], ascending = False, axis = 1, inplace = True)\n",
    "    \n",
    "#=========================    \n",
    "# get event actions as a % of sessions\n",
    "#=========================\n",
    "\n",
    "    a2 = a1.div(report_dict[account_name + ' overall over time']['sessions'].values, axis = 0)\n",
    "    \n",
    "    report_dict[account_name + ' checkouts % over time'] = a4    \n",
    "    \n",
    "#=========================\n",
    "# get confidence interval\n",
    "#=========================    \n",
    "    \n",
    "    conf = jb_conf(a2).loc[:'upper'].T\n",
    "    \n",
    "    report_dict[account_name + ' checkouts % conf'] = conf\n",
    "    \n",
    "#=========================\n",
    "# get trend\n",
    "#=========================    \n",
    "    \n",
    "    a3 = a2.T\n",
    "    vals = a3.values\n",
    "    ls = []\n",
    "    for i in range(len(vals)):\n",
    "        slope = get_trend(vals[i])\n",
    "        results = 'up' if slope > 0 else 'down'\n",
    "        ls.append(results)    \n",
    "        \n",
    "#=========================\n",
    "# isolate most recent week\n",
    "#=========================    \n",
    "\n",
    "    a4 = pd.DataFrame(a3.iloc[:,-1])\n",
    "    a4.columns = ['week ending ' + x for x in a4.columns]\n",
    "    \n",
    "#=========================\n",
    "# add in avg and trend\n",
    "#=========================    \n",
    "\n",
    "    a4['average'] = conf['mean']\n",
    "    a4['week to week trend'] = ls\n",
    "    \n",
    "#=========================\n",
    "# add in confidence interval results\n",
    "#=========================    \n",
    "\n",
    "    ls = []\n",
    "    for index, row in a4.iterrows():\n",
    "        lower, mean, upper = conf.loc[index].values\n",
    "        val = row.values[0]\n",
    "        if np.logical_and(val >= lower, val <= upper):\n",
    "            res = 'normal'\n",
    "        else:\n",
    "            if val < lower:\n",
    "                res = 'BELOW normal'\n",
    "            else:\n",
    "                res = 'ABOVE normal'\n",
    "                \n",
    "        ls.append(res)\n",
    "        \n",
    "    a4['comment on current value'] = ls\n",
    "    \n",
    "#=========================\n",
    "# add to dict\n",
    "#=========================        \n",
    "       \n",
    "    report_dict[account_name + ' checkouts % current'] = a4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "www.adafruit.com data is from 2018-12-03 to 2019-03-03 (90 days)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Week Ending 2019-03-03</th>\n",
       "      <th>Average</th>\n",
       "      <th>Week To Week Trend</th>\n",
       "      <th>Comment On Current Value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>event action</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>checkout</th>\n",
       "      <td>12.6%</td>\n",
       "      <td>0.120237</td>\n",
       "      <td>up</td>\n",
       "      <td>ABOVE normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>add to cart</th>\n",
       "      <td>10.1%</td>\n",
       "      <td>0.098240</td>\n",
       "      <td>up</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>purchase</th>\n",
       "      <td>1.3%</td>\n",
       "      <td>0.012916</td>\n",
       "      <td>up</td>\n",
       "      <td>ABOVE normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Week Ending 2019-03-03   Average Week To Week Trend  \\\n",
       "event action                                                       \n",
       "checkout                      12.6%  0.120237                 up   \n",
       "add to cart                   10.1%  0.098240                 up   \n",
       "purchase                       1.3%  0.012916                 up   \n",
       "\n",
       "             Comment On Current Value  \n",
       "event action                           \n",
       "checkout                 ABOVE normal  \n",
       "add to cart                    normal  \n",
       "purchase                 ABOVE normal  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 'www.adafruit.com'\n",
    "\n",
    "print('%s data is from %s' % (a, report_dict[a + ' checkouts period']))\n",
    "\n",
    "df = report_dict[a + ' checkouts % current']\n",
    "fmt = ['p1',0,0]\n",
    "df.format_(fmt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*****\n",
    "# Get exit page data\n",
    "*****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "HttpError",
     "evalue": "<HttpError 403 when requesting https://www.googleapis.com/analytics/v3/data/ga?ids=ga%3A15556579&dimensions=ga%3Adate%2Cga%3Ahostname%2Cga%3AexitPagePath&metrics=ga%3Apageviews&start-date=2019-02-28&end-date=2019-02-28&start-index=1&max-results=10000&alt=json returned \"User Rate Limit Exceeded. Rate of requests for user exceed configured project quota. You may consider re-evaluating expected per-user traffic to the API and adjust project quota limits accordingly. You may monitor aggregate quota usage and adjust limits in the API Console: https://console.developers.google.com/apis/api/analytics.googleapis.com/quotas?project=173080773975\">",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHttpError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-fa42c9bbba64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0;34m'pageviews'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m                 None)\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mexit_super_main\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mexit_super_main\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Fake Folder/Python Libraries/jb_google_analytics.py\u001b[0m in \u001b[0;36mget_ga\u001b[0;34m(account_id, date_start, date_end, dimensions, metrics, filters)\u001b[0m\n\u001b[1;32m     28\u001b[0m                      'filters':filters}\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0minter_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m             \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minter_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/google2pandas/_panalysis_ga.py\u001b[0m in \u001b[0;36mexecute_query\u001b[0;34m(self, as_dict, all_results, **query)\u001b[0m\n\u001b[1;32m    256\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Error making query: {0}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mga_query\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# Fix the 'query' field to be useful to us\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/oauth2client/_helpers.py\u001b[0m in \u001b[0;36mpositional_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mpositional_parameters_enforcement\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mPOSITIONAL_WARNING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpositional_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/googleapiclient/http.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, http, num_retries)\u001b[0m\n\u001b[1;32m    842\u001b[0m       \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 844\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mHttpError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muri\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    845\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostproc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHttpError\u001b[0m: <HttpError 403 when requesting https://www.googleapis.com/analytics/v3/data/ga?ids=ga%3A15556579&dimensions=ga%3Adate%2Cga%3Ahostname%2Cga%3AexitPagePath&metrics=ga%3Apageviews&start-date=2019-02-28&end-date=2019-02-28&start-index=1&max-results=10000&alt=json returned \"User Rate Limit Exceeded. Rate of requests for user exceed configured project quota. You may consider re-evaluating expected per-user traffic to the API and adjust project quota limits accordingly. You may monitor aggregate quota usage and adjust limits in the API Console: https://console.developers.google.com/apis/api/analytics.googleapis.com/quotas?project=173080773975\">"
     ]
    }
   ],
   "source": [
    "s = dt.datetime.now()\n",
    "\n",
    "exit_super_main = pd.DataFrame()\n",
    "\n",
    "for k in accounts.keys():\n",
    "    \n",
    "    df = get_ga(k,\n",
    "                four_weeks_ago,\n",
    "                date_end,\n",
    "                \n",
    "                ['date',\n",
    "                 'hostname',\n",
    "                 'exitPagePath'], \n",
    "                \n",
    "                ['pageviews'],\n",
    "               \n",
    "                None)\n",
    "    \n",
    "    exit_super_main = pd.concat([exit_super_main, df], sort = False, ignore_index = True)\n",
    "    \n",
    "exit_super_main['account name'] = exit_super_main['account id'].map(accounts)    \n",
    "exit_super_main.rename(columns = {'exit page path':'exit page'}, inplace = True)\n",
    "\n",
    "# get week ending, using my function\n",
    "df = jb_week_ending(exit_super_main, 'date', 'Sunday')\n",
    "exit_super_main = pd.merge(exit_super_main, df, on = 'date')\n",
    "\n",
    "e = dt.datetime.now()\n",
    "print(e-s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exit_main = exit_super_main.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up exit page data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = dt.datetime.now()\n",
    "\n",
    "exit_main['exit page clean'] = exit_main['exit page'].apply(page_clean)\n",
    "exit_main['exit page clean'].fillna(exit_main['hostname'], inplace = True)\n",
    "\n",
    "e = dt.datetime.now()\n",
    "print(e-s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = exit_main['date'].min().date()\n",
    "d2 = exit_main['date'].max().date()\n",
    "days = (pd.to_datetime(d2) - pd.to_datetime(d1)).days\n",
    "\n",
    "for account_name in set(exit_main['account name']):\n",
    "    \n",
    "    report_dict[account_name + ' exit pages period'] = '%s to %s (%i days)' % (d1,d2,days)\n",
    "\n",
    "#=========================\n",
    "# aggregate data over entire period\n",
    "#=========================\n",
    "    \n",
    "    a = exit_main[exit_main['account name'] == account_name].groupby('exit page clean')[['pageviews']].sum()\n",
    "    a.sort_values('pageviews', ascending = False, inplace = True)\n",
    "    a['% of total pageviews'] = a['pageviews']/a['pageviews'].sum()\n",
    "    a['% running sum'] = a['% of total pageviews'].cumsum()\n",
    "\n",
    "#=========================\n",
    "# get top X% and bottom 1-X%\n",
    "#=========================    \n",
    "    \n",
    "    q = 0.95\n",
    "    \n",
    "    top = a[a['% running sum'] <= q]\n",
    "    bottom = a[a['% running sum'] > q]\n",
    "    \n",
    "    report_dict[account_name + ' exit pages top'] = top\n",
    "    report_dict[account_name + ' exit pages bottom'] = bottom\n",
    "        \n",
    "    top_exits = top.index.tolist()\n",
    "\n",
    "#=========================\n",
    "# take top exits, group by period\n",
    "#=========================    \n",
    "\n",
    "    b1 = exit_main[(exit_main['account name'] == account_name)\n",
    "                 & (exit_main['exit page clean'].isin(top_exits))].groupby([report_type,'exit page clean'])[['pageviews']].sum().unstack(1)\n",
    "    b1.columns = b1.columns.droplevel(0)\n",
    "    b1.sort_values(b1.index[-1], ascending = False, axis = 1, inplace = True)\n",
    "\n",
    "# dont use raw counts, use \"exits as % of pageviews\", below\n",
    "# if some page has more views it could have more exits\n",
    "# raw counts will be misleading, while % won't be \n",
    "       \n",
    "#=========================\n",
    "# exits as % of pageviews\n",
    "#=========================    \n",
    "\n",
    "    total_pageviews = report_dict[account_name + ' overall over time'].loc[b1.index][['pageviews']]\n",
    "    total_pageviews.columns = ['total pageviews']    \n",
    "\n",
    "    b2 = b1.div(total_pageviews.values.flatten(), axis = 0)\n",
    "    \n",
    "    report_dict[account_name + ' exit pages % over time'] = b2\n",
    "    \n",
    "#=========================\n",
    "# get confidence interval\n",
    "#=========================    \n",
    "\n",
    "    conf = jb_conf(b2).loc[:'upper'].T\n",
    "    \n",
    "    report_dict[account_name + ' exit pages % conf'] = conf\n",
    "    \n",
    "#=========================\n",
    "# isolate most recent week\n",
    "#=========================    \n",
    "\n",
    "    b3 = b2.T.iloc[:,-1:]\n",
    "    b3.columns = ['week ending ' + x for x in b3.columns]   \n",
    "    \n",
    "    b3['average'] = conf['mean']\n",
    "    \n",
    "    b3['% running sum'] = b3.iloc[:,0].cumsum()\n",
    "    \n",
    "#=========================\n",
    "# get trend\n",
    "#=========================    \n",
    "    \n",
    "    vals = b2.T.values\n",
    "    ls = []\n",
    "    for i in range(len(vals)):\n",
    "        slope = get_trend(vals[i])\n",
    "        results = 'up' if slope > 0 else 'down'\n",
    "        ls.append(results)        \n",
    "        \n",
    "    b3['week to week trend'] = ls\n",
    "        \n",
    "#=========================\n",
    "# add in confidence interval results\n",
    "#=========================    \n",
    "\n",
    "    ls = []\n",
    "    for index, row in b3.iterrows():\n",
    "        lower, mean, upper = conf.loc[index].values\n",
    "        val = row.values[0]\n",
    "        if np.logical_and(val >= lower, val <= upper):\n",
    "            res = 'normal'\n",
    "        else:\n",
    "            if val < lower:\n",
    "                res = 'BELOW normal'\n",
    "            else:\n",
    "                res = 'ABOVE normal'\n",
    "                \n",
    "        ls.append(res)\n",
    "        \n",
    "    b3['comment on current value'] = ls       \n",
    "    \n",
    "    report_dict[account_name + ' exit pages % current'] = b3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 'www.adafruit.com'\n",
    "\n",
    "print('%s data is from %s' % (a, report_dict[a + ' exit pages period']))\n",
    "\n",
    "df = report_dict[a + ' exit pages % current']\n",
    "fmt = ['p1','p1',0,0]\n",
    "df.format_(fmt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*****\n",
    "# Get text search data\n",
    "*****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = dt.datetime.now()\n",
    "\n",
    "searches_super_main = pd.DataFrame()\n",
    "\n",
    "for k in accounts.keys():\n",
    "    \n",
    "    try:\n",
    "        df = get_ga(k,\n",
    "                    four_weeks_ago,\n",
    "                    date_end,\n",
    "\n",
    "                    ['date',\n",
    "                     'searchKeyword',\n",
    "                     'searchKeywordRefinement',\n",
    "                     'hostname',\n",
    "                     'searchStartPage',\n",
    "\n",
    "                     # the page users immediately visited after performing an internal search on the site. This is usually the search results page.                 \n",
    "                     # this is always \"/search\", or \"/?q=\", etc, i.e. it doesn't give any meaningful info, so omit it\n",
    "                     #'searchDestinationPage', \n",
    "\n",
    "                     # the page that users visited after performing an internal search on the site.\n",
    "                     'searchAfterDestinationPage',\n",
    "                     'exitPagePath'], \n",
    "\n",
    "                    ['searchResultViews', # the number of times a search result page was viewed\n",
    "                     'searchRefinements'],\n",
    "                   \n",
    "                    None)\n",
    "\n",
    "        searches_super_main = pd.concat([searches_super_main, df], sort = False, ignore_index = True)\n",
    "    \n",
    "    except:\n",
    "        print(accounts[k] + ' does not have any GA data\\n')\n",
    "        \n",
    "searches_super_main['account name'] = searches_super_main['account id'].map(accounts)    \n",
    "        \n",
    "# get week ending, using my function\n",
    "df = jb_week_ending(searches_super_main, 'date', 'Sunday')\n",
    "searches_super_main = pd.merge(searches_super_main, df, on = 'date')\n",
    "\n",
    "e = dt.datetime.now()\n",
    "print(e-s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searches_super_main.rename(columns = {'search keyword refinement':'refined to',\n",
    "                                      'search refinements':'refinements',\n",
    "                                      'search result views':'search results',\n",
    "                                      'search start page':'start page',\n",
    "                                      'search after destination page':'page 01',\n",
    "                                      'exit page path':'exit page'}, inplace = True)\n",
    "\n",
    "old_cols = list(searches_super_main.columns)\n",
    "new_cols = ['date',\n",
    "            'year and month',\n",
    "            'year and quarter',\n",
    "            'year',\n",
    "            'week ending',\n",
    "            'search keyword',\n",
    "            'search results',\n",
    "            'refined to',\n",
    "            'refinements',\n",
    "            'account id',\n",
    "            'account name',\n",
    "            'hostname',\n",
    "            'start page',\n",
    "            'page 01',\n",
    "            'exit page']\n",
    "\n",
    "s1 = set(new_cols)\n",
    "s2 = set(old_cols)\n",
    "s3 = s1.symmetric_difference(s2)\n",
    "\n",
    "if len(s3) > 0:\n",
    "    print(s3)\n",
    "    raise ValueError('check ur columns')\n",
    "    \n",
    "else:\n",
    "    searches_super_main = searches_super_main[new_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searches_main = searches_super_main.copy()\n",
    "\n",
    "searches_main['search keyword clean'] = [re.sub(r'([^\\s\\w.])+', '', x) for x in searches_main['search keyword']]\n",
    "searches_main.drop(searches_main[searches_main['search keyword clean'] == ''].index, inplace = True)\n",
    "searches_main.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up page results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = dt.datetime.now()\n",
    "\n",
    "cols = ['page 01','exit page']\n",
    "for col in cols:\n",
    "    try:\n",
    "        searches_main[col + ' clean'].drop(axis = 1, inplace = True)\n",
    "    except:\n",
    "        searches_main[col + ' clean'] = searches_main[col].apply(page_clean)\n",
    "        searches_main[col + ' clean'].fillna(value = pd.np.nan, inplace = True)        \n",
    "        searches_main[col + ' clean'].fillna(searches_main['hostname'], inplace = True)\n",
    "        \n",
    "e = dt.datetime.now()\n",
    "print(e-s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove top X% shortest and longest words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searches_main['keyword length'] = searches_main['search keyword'].str.len()\n",
    "\n",
    "print(searches_main['keyword length'].quantile(np.arange(0,1.05,0.05)))\n",
    "\n",
    "q = [0.05,0.95]\n",
    "\n",
    "searches_main.drop(searches_main[(searches_main['keyword length'] < searches_main['keyword length'].quantile(q[0]))\n",
    "             | (searches_main['keyword length'] > searches_main['keyword length'].quantile(q[1]))].index, inplace = True)\n",
    "\n",
    "searches_main.reset_index(drop = True, inplace = True)\n",
    "\n",
    "# drop the length column\n",
    "searches_main.drop('keyword length',1,inplace = True)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove words that are just numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_this = 'no'\n",
    "\n",
    "if do_this == 'yes':\n",
    "    a = len(searches_main[searches_main['search keyword'].str.isdigit()])/len(searches_main)\n",
    "    print('{:,.2f}% of lines contain search keywords which are only numbers\\nremove these'.format(a * 100))\n",
    "\n",
    "    searches_main.drop(searches_main[searches_main['search keyword'].str.isdigit()].index, inplace = True)\n",
    "    searches_main.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rate the similarity of strings\n",
    "* Use Jaccard Index: https://en.wikipedia.org/wiki/Jaccard_index\n",
    "* And difflib.SequenceMatcher: http://epydoc.sourceforge.net/stdlib/difflib-module.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "\n",
    "def jacc(a,b):\n",
    "    \n",
    "    s1 = set(a)\n",
    "    s2 = set(b)\n",
    "    inter = s1.intersection(s2)\n",
    "    union = s1.union(s2)\n",
    "    score = len(inter)/len(union)    \n",
    "    \n",
    "    return score\n",
    "\n",
    "def get_score(a,b):\n",
    "    \n",
    "    score1 = SequenceMatcher(a = a, b = b).ratio()\n",
    "    \n",
    "    s1 = set(a)\n",
    "    s2 = set(b)\n",
    "    inter = s1.intersection(s2)\n",
    "    union = s1.union(s2)\n",
    "    score2 = len(inter)/len(union)\n",
    "    \n",
    "    avg = np.mean([score1, score2])\n",
    "    \n",
    "    return avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get part categories to use as top words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "categories = pd.read_sql(\n",
    "'''\n",
    "SELECT\n",
    "DISTINCT LOWER(categories_name) AS categories_name\n",
    "FROM categories_description\n",
    "''', db)\n",
    "\n",
    "col_fix(categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rate all search keywords against category names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s = dt.datetime.now()\n",
    "\n",
    "top_words = list(categories['categories name'])\n",
    "all_words = list(set(searches_main['search keyword clean']))\n",
    "\n",
    "ls = []\n",
    "for a in all_words:\n",
    "    score = 0\n",
    "    for t in top_words:\n",
    "        inter_score = jacc(a.replace(' ',''), t.replace(' ','')) # compare words without their whitespace\n",
    "        if inter_score > score:\n",
    "            score = inter_score\n",
    "            t2 = t\n",
    "    ls.append([t2,a,score])\n",
    "    \n",
    "cols = ['top word','search keyword','score']\n",
    "scores = pd.DataFrame(data = ls, columns = cols)\n",
    "\n",
    "e = dt.datetime.now()\n",
    "print(e-s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review test words and their scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "do_this = 'yes'\n",
    "\n",
    "test_words = ['arduino','raspberry pi','feather']\n",
    "\n",
    "scores_dict = {}\n",
    "for t in test_words:\n",
    "    scores_dict[t] = scores[scores['top word'] == t].sort_values('score', ascending = False)\n",
    "    \n",
    "    if do_this == 'no':\n",
    "        \n",
    "        fmt = [0,0,'p1']\n",
    "        display(scores_dict[t].head(10).format_(fmt))\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describe scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "des = scores[['score']].describe().T\n",
    "fmt = ['n0','p1','n2','p1','p1','p1','p1','p1']\n",
    "des = des.format_(fmt).T\n",
    "des"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map top words to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['top word','score']:\n",
    "    searches_main[col] = searches_main['search keyword'].map(dict(zip(scores['search keyword'], scores[col])))\n",
    "\n",
    "cutoff = 0.40\n",
    "\n",
    "def top_word_clean(df):\n",
    "    if df['search keyword'].isdigit():\n",
    "        return 'pn search'\n",
    "    else:\n",
    "        if pd.isnull(df['top word']):\n",
    "            return df['search keyword']\n",
    "        else:\n",
    "            if df['score'] < cutoff:\n",
    "                return 'uncategorizable (score cutoff: {:,.0f}%)'.format(cutoff * 100)\n",
    "            else:\n",
    "                return df['top word']\n",
    "        \n",
    "searches_main['top word'] = searches_main.apply(top_word_clean, axis = 1)\n",
    "\n",
    "if searches_main[searches_main['top word'].isnull()].empty == False:\n",
    "    raise VaueError('check your top_word nulls')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "d1 = searches_main['date'].min().date()\n",
    "d2 = searches_main['date'].max().date()\n",
    "days = (pd.to_datetime(d2) - pd.to_datetime(d1)).days\n",
    "\n",
    "N = 10\n",
    "      \n",
    "for account_name in list(set(searches_main['account name'])):\n",
    "    \n",
    "    report_dict[account_name + ' search keyword period'] = '%s to %s (%i days)' % (d1,d2,days)    \n",
    "\n",
    "#=========================\n",
    "# by top word\n",
    "#=========================\n",
    "    \n",
    "    a1 = searches_main[searches_main['account name'] == account_name]\n",
    "    a2 = a1.groupby('top word')[['search results']].sum()\n",
    "    a2.sort_values('search results', ascending = False, inplace = True)\n",
    "    a3 = a2.iloc[:N].copy()\n",
    "    \n",
    "    top_words = a3.index.tolist()\n",
    "    total1 = a1['search results'].sum()\n",
    "    \n",
    "    a3['% of \"account name\"'] = a3['search results']/total1\n",
    "    a3['% of \"account name\" running sum'] = a3['% of \"account name\"'].cumsum()\n",
    "    \n",
    "    b1 = searches_main[(searches_main['account name'] == account_name) & (searches_main['top word'].isin(top_words))]\n",
    "    b2 = b1.groupby(['top word', report_type])[['search results']].sum().unstack(1).fillna(0)\n",
    "    b2.columns = b2.columns.droplevel(0)    \n",
    "    b2 = b2.loc[top_words]\n",
    "    \n",
    "    vals = b2.values\n",
    "\n",
    "    ls = []\n",
    "    for i in range(len(vals)):\n",
    "        trend = get_trend(vals[i])\n",
    "        res = 'up' if trend > 0 else 'down'\n",
    "        ls.append(res)    \n",
    "        \n",
    "    a3['trending'] = ls\n",
    "    \n",
    "    report_dict[account_name + ' by top word'] = a3\n",
    "    \n",
    "#=========================\n",
    "# by keyword search\n",
    "#=========================    \n",
    "    \n",
    "    c1 = searches_main[(searches_main['account name'] == account_name)\n",
    "                     & (searches_main['top word'].isin(top_words))]\n",
    "    c2 = c1.groupby(['top word','search keyword'])[['search results']].sum()\n",
    "    c3 = c2['search results'].groupby(level = 0, group_keys = False)\n",
    "    c4 = c3.apply(lambda x: x.sort_values(ascending = False).head(N))\n",
    "    c5 = pd.DataFrame(c4)\n",
    "    \n",
    "    total2 = a2.reset_index()\n",
    "    total2.rename(columns = {'search results':'total'}, inplace = True)    \n",
    "    \n",
    "    c6 = c5.reset_index().merge(total2, on = ['top word'])\n",
    "    c6['% of \"top words\"'] = c6['search results']/c6['total']\n",
    "    c6.sort_values(['total','search results'], ascending = [False,False], inplace = True)\n",
    "    c6.drop('total', 1, inplace = True)   \n",
    "    c7 = c6.set_index(['top word','search keyword'])\n",
    "    \n",
    "    report_dict[account_name + ' by search keyword'] = c7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = 'www.adafruit.com'\n",
    "show_this = 'no'\n",
    "\n",
    "if show_this == 'yes':\n",
    "\n",
    "    print('%s is from %s\\n' % (a, report_dict[a + ' search keyword period']))\n",
    "    print('by top word')\n",
    "    fmt = ['n0','p1','p2',0]\n",
    "    display(report_dict[a + ' by top word'].format_(fmt))\n",
    "\n",
    "    print('by keyword')\n",
    "    fmt = ['n0','p1']\n",
    "    display(report_dict[a + ' by search keyword'].format_(fmt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***** \n",
    "# Excel Start\n",
    "*****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup workbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workbook = xlsxwriter.Workbook(csv_path + workbook_title + '.xlsx',\n",
    "                               {'nan_inf_to_errors': True,\n",
    "                               'strings_to_numbers': True})\n",
    "\n",
    "\n",
    "tabs = ['Summary',\n",
    "        'Overall Stats',\n",
    "        'Checkout Stats',\n",
    "        'Exit Page Stats',\n",
    "        'Searches by Top Word',\n",
    "        'Searches by Keyword',\n",
    "        'How We Define Top Words']\n",
    "\n",
    "#=========================\n",
    "# Formats\n",
    "#=========================\n",
    "\n",
    "colors = ['#343635',\n",
    "          '#2e4874',\n",
    "          '#7eaba4',\n",
    "          '#928c85',\n",
    "          '#347c83',\n",
    "          '#bfb9d6']\n",
    "colors = colors * 2\n",
    "\n",
    "title = workbook.add_format({'font_size':25,\n",
    "                             'font_name':'Arial (Bold)'})\n",
    "\n",
    "subtitle = workbook.add_format({'font_size':15,\n",
    "                                'font_name':'Arial (Bold)'})\n",
    "\n",
    "\n",
    "col_names = workbook.add_format({'font_name':'Arial (Bold)',\n",
    "                                 'font_color':'white',\n",
    "                                 'valign':'vcenter',\n",
    "                                 'align':'center',\n",
    "                                 'bg_color':colors[2],\n",
    "                                 'bottom':1,\n",
    "                                 'top':1,\n",
    "                                 'left':1,\n",
    "                                 'right':1})\n",
    "\n",
    "money = workbook.add_format({'num_format':'$#,##0', 'align':'center'})\n",
    "money2 = workbook.add_format({'num_format':'$#,##0.00', 'align':'center'})\n",
    "percent = workbook.add_format({'num_format':'0.0%', 'align':'center'})\n",
    "number = workbook.add_format({'num_format':'#,##0', 'align':'center'})\n",
    "number1 = workbook.add_format({'num_format':'#,##0.0', 'align':'center'})\n",
    "dummy = workbook.add_format({'font_color':'black', 'align':'center'})\n",
    "\n",
    "for tab in tabs:\n",
    "    workbook.add_worksheet(tab) # create each tab\n",
    "\n",
    "my_worksheets = {}\n",
    "for sht in workbook.worksheets():\n",
    "    my_worksheets[sht.get_name()] = sht # create dict like tab_name:worksheet_instance\n",
    "\n",
    "#=========================\n",
    "# Apply same formatting to each worksheet\n",
    "#=========================\n",
    "\n",
    "for k, v in my_worksheets.items():\n",
    "    sht = my_worksheets[k]\n",
    "    sht.write(0, 0,\n",
    "              k.title(),\n",
    "              title)\n",
    "    sht.write(1, 0,\n",
    "              workbook_title,\n",
    "              subtitle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### DO THIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sht = my_worksheets['Overall Stats']\n",
    "\n",
    "fmts = {'Users':number,\n",
    "       'Sessions':number,\n",
    "       'Unique Searches':number,\n",
    "       'Sessions With Search':number,\n",
    "       'Search Exits As % Of Unique Searches':percent,\n",
    "       'Search Exits':number,\n",
    "       'Avg Search Per Search Session':number1,\n",
    "       '% Of Sessions With Search':percent,\n",
    "       'Pageviews':number,\n",
    "       'Avg Pageview Per Session':number1}\n",
    "\n",
    "keys = []\n",
    "for k in report_dict.keys():\n",
    "    if 'overall current' in k:\n",
    "        keys.append(k)\n",
    "\n",
    "start_row = 3\n",
    "start_col = 0\n",
    "\n",
    "sht.write(start_row - 1,\n",
    "          start_col,\n",
    "         'Data is from ' + report_dict[k.split(' ')[0] + ' overall period'])\n",
    "        \n",
    "for k in keys:\n",
    "    \n",
    "    df = report_dict[k].copy()\n",
    "    df.reset_index(inplace = True)\n",
    "    df.rename(columns = {'index':'metric'}, inplace = True)\n",
    "\n",
    "    df.columns = df.columns.str.title()\n",
    "\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'O':\n",
    "            df[col] = df[col].str.title()\n",
    "            \n",
    "    sht.write(start_row + 1,\n",
    "             start_col,\n",
    "             k.split(' ')[0].title(),\n",
    "             subtitle)\n",
    "            \n",
    "    for i in range(len(df.columns)):\n",
    "        \n",
    "        col_width = []        \n",
    "        \n",
    "        sht.write(start_row + 2,\n",
    "                 start_col + i,\n",
    "                 df.columns[i],\n",
    "                 col_names)\n",
    "        \n",
    "        col_width.append(len(df.columns[i]))\n",
    "                \n",
    "        for j in range(len(df)):            \n",
    "            \n",
    "            if df.columns[i] in ['Week Ending %s' % date_end,'Average']:\n",
    "                fmt = fmts[df.iloc[j]['Metric']]\n",
    "            else:\n",
    "                fmt = dummy            \n",
    "            \n",
    "            sht.write(start_row + 3 + j,\n",
    "                     start_col + i,\n",
    "                     df.iloc[j,i],\n",
    "                     fmt)\n",
    "            \n",
    "            col_width.append(len(str(df.iloc[j,i])))\n",
    "            \n",
    "        sht.set_column(start_col + i,\n",
    "                       start_col + i,\n",
    "                       np.max(col_width) + 1)\n",
    "        \n",
    "    start_col += len(df.columns) + 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkout Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sht = my_worksheets['Checkout Stats']\n",
    "\n",
    "fmts = {'Checkout':percent,\n",
    "        'Add To Cart':percent,\n",
    "        'Purchase':percent}\n",
    "\n",
    "keys = []\n",
    "for k in report_dict.keys():\n",
    "    if 'checkouts % current' in k:\n",
    "        keys.append(k)\n",
    "        \n",
    "start_row = 3\n",
    "start_col = 0       \n",
    "\n",
    "sht.write(start_row - 1,\n",
    "          start_col,\n",
    "         'Data is from ' + report_dict[k.split(' ')[0] + ' checkouts period'] + ' and is \"Event Action as a % of Sessions\"')\n",
    "        \n",
    "for k in keys:\n",
    "    \n",
    "    df = report_dict[k].copy()\n",
    "    df.reset_index(inplace = True)\n",
    "\n",
    "    df.columns = df.columns.str.title()\n",
    "\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'O':\n",
    "            df[col] = df[col].str.title()        \n",
    "            \n",
    "    sht.write(start_row + 1,\n",
    "              start_col,\n",
    "              k.split(' ')[0].title(),\n",
    "              subtitle)\n",
    "                \n",
    "    for i in range(len(df.columns)):\n",
    "        \n",
    "        col_width = []        \n",
    "        \n",
    "        sht.write(start_row + 2,\n",
    "                 start_col + i,\n",
    "                 df.columns[i],\n",
    "                 col_names)\n",
    "        \n",
    "        col_width.append(len(df.columns[i]))\n",
    "                \n",
    "        for j in range(len(df)):            \n",
    "            \n",
    "            if df.columns[i] in ['Week Ending %s' % date_end,'Average']:\n",
    "                fmt = fmts[df.iloc[j]['Event Action']]\n",
    "            else:\n",
    "                fmt = dummy            \n",
    "            \n",
    "            sht.write(start_row + 3 + j,\n",
    "                     start_col + i,\n",
    "                     df.iloc[j,i],\n",
    "                     fmt)\n",
    "            \n",
    "            col_width.append(len(str(df.iloc[j,i])))\n",
    "            \n",
    "        sht.set_column(start_col + i,\n",
    "                       start_col + i,\n",
    "                       np.max(col_width) + 1)\n",
    "        \n",
    "    start_col += len(df.columns) + 2    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exit Pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sht = my_worksheets['Exit Page Stats']\n",
    "\n",
    "keys = []\n",
    "for k in report_dict.keys():\n",
    "    if 'exit pages % current' in k:\n",
    "        keys.append(k)\n",
    "        \n",
    "start_row = 3\n",
    "start_col = 0       \n",
    "\n",
    "sht.write(start_row - 1,\n",
    "          start_col,\n",
    "         'Data is from ' + report_dict[k.split(' ')[0] + ' exit pages period'])\n",
    "        \n",
    "for k in keys:\n",
    "    \n",
    "    df = report_dict[k].copy()\n",
    "    df.reset_index(inplace = True)\n",
    "    df.rename(columns = {'exit page clean':'exit page'}, inplace = True)\n",
    "\n",
    "    df.columns = df.columns.str.title()\n",
    "\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'O':\n",
    "            df[col] = df[col].str.title()        \n",
    "            \n",
    "    sht.write(start_row + 1,\n",
    "              start_col,\n",
    "              k.split(' ')[0].title(),\n",
    "              subtitle)\n",
    "                \n",
    "    for i in range(len(df.columns)):\n",
    "        \n",
    "        col_width = []        \n",
    "        \n",
    "        sht.write(start_row + 2,\n",
    "                 start_col + i,\n",
    "                 df.columns[i],\n",
    "                 col_names)\n",
    "        \n",
    "        col_width.append(len(df.columns[i]))\n",
    "                \n",
    "        for j in range(len(df)):            \n",
    "            \n",
    "            if df.columns[i] in ['Week Ending %s' % date_end,'Average','% Running Sum']:\n",
    "                fmt = percent\n",
    "            else:\n",
    "                fmt = dummy            \n",
    "            \n",
    "            sht.write(start_row + 3 + j,\n",
    "                     start_col + i,\n",
    "                     df.iloc[j,i],\n",
    "                     fmt)\n",
    "            \n",
    "            col_width.append(len(str(df.iloc[j,i])))\n",
    "            \n",
    "        sht.set_column(start_col + i,\n",
    "                       start_col + i,\n",
    "                       np.max(col_width) + 1)\n",
    "        \n",
    "    start_col += len(df.columns) + 2    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searches By Top Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sht = my_worksheets['Searches by Top Word']\n",
    "\n",
    "start_row = 3\n",
    "start_col = 0       \n",
    "\n",
    "keys = []\n",
    "for k in report_dict.keys():\n",
    "    if ' by top word' in k:\n",
    "        keys.append(k)\n",
    "        \n",
    "fmts = {'Top Word':dummy,\n",
    "        'Search Results':number,\n",
    "        '% Of \"Account Name\"':percent,\n",
    "        '% Of \"Account Name\" Running Sum':percent,\n",
    "        'Trending':dummy}        \n",
    "        \n",
    "sht.write(start_row - 1,\n",
    "          start_col,\n",
    "         'Data is from ' + report_dict[k.split(' ')[0] + ' search keyword period'])\n",
    "\n",
    "for k in keys:\n",
    "    \n",
    "    df = report_dict[k].copy()\n",
    "    df.reset_index(inplace = True)\n",
    "\n",
    "    df.columns = df.columns.str.title()\n",
    "\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'O':\n",
    "            df[col] = df[col].str.title()        \n",
    "            \n",
    "    sht.write(start_row + 1,\n",
    "              start_col,\n",
    "              k.split(' ')[0].title(),\n",
    "              subtitle)\n",
    "                \n",
    "    for i in range(len(df.columns)):\n",
    "        \n",
    "        col_width = []        \n",
    "        \n",
    "        sht.write(start_row + 2,\n",
    "                 start_col + i,\n",
    "                 df.columns[i],\n",
    "                 col_names)\n",
    "        \n",
    "        col_width.append(len(df.columns[i]))\n",
    "                \n",
    "        for j in range(len(df)):                        \n",
    "            sht.write(start_row + 3 + j,\n",
    "                     start_col + i,\n",
    "                     df.iloc[j,i],\n",
    "                     fmts[df.columns[i]])\n",
    "            \n",
    "            col_width.append(len(str(df.iloc[j,i])))\n",
    "            \n",
    "        sht.set_column(start_col + i,\n",
    "                       start_col + i,\n",
    "                       np.max(col_width) + 1)\n",
    "        \n",
    "    start_col += len(df.columns) + 2    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searches by Keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sht = my_worksheets['Searches by Keyword']\n",
    "\n",
    "fmts = {'Top Word':dummy,\n",
    "       'Search Keyword':dummy,\n",
    "       'Search Results':number,\n",
    "       '% Of \"Top Words\"':percent}\n",
    "\n",
    "start_row = 3\n",
    "start_col = 0       \n",
    "\n",
    "keys = []\n",
    "for k in report_dict.keys():\n",
    "    if ' by search keyword' in k:\n",
    "        keys.append(k)\n",
    "        \n",
    "sht.write(start_row - 1,\n",
    "          start_col,\n",
    "         'Data is from ' + report_dict[k.split(' ')[0] + ' search keyword period'])        \n",
    "        \n",
    "for k in keys:\n",
    "    \n",
    "    df = report_dict[k].copy()\n",
    "    df.reset_index(inplace = True)\n",
    "\n",
    "    df.columns = df.columns.str.title()\n",
    "\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'O':\n",
    "            df[col] = df[col].str.title()       \n",
    "            \n",
    "    sht.write(start_row + 1,\n",
    "              start_col,\n",
    "              k.split(' ')[0].title(),\n",
    "              subtitle)            \n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        if i%10 != 0:\n",
    "            df.loc[i, 'Top Word'] = ''    \n",
    "\n",
    "    for i in range(len(df.columns)):\n",
    "        \n",
    "        col_width = []        \n",
    "        \n",
    "        sht.write(start_row + 2,\n",
    "                 start_col + i,\n",
    "                 df.columns[i],\n",
    "                 col_names)\n",
    "        \n",
    "        col_width.append(len(df.columns[i]))\n",
    "                \n",
    "        for j in range(len(df)):                        \n",
    "            sht.write(start_row + 3 + j,\n",
    "                     start_col + i,\n",
    "                     df.iloc[j,i],\n",
    "                     fmts[df.columns[i]])\n",
    "            \n",
    "            col_width.append(len(str(df.iloc[j,i])))\n",
    "                        \n",
    "        sht.set_column(start_col + i,\n",
    "                       start_col + i,\n",
    "                       np.max(col_width) + 1)\n",
    "        \n",
    "    start_col += len(df.columns) + 2                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How We Define Top Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sht = my_worksheets['How We Define Top Words']\n",
    "\n",
    "text = open('/Users/jarad/Fake Folder/New Products/Recurring/Algolia Searches Report/Docs/textbox.txt', 'r').read()\n",
    "options = {'width': 510,\n",
    "           'height': 280}\n",
    "\n",
    "start_row = 3\n",
    "start_col = 0\n",
    "\n",
    "sht.insert_textbox(start_row + 1,\n",
    "                   start_col,\n",
    "                   text,\n",
    "                   options)\n",
    "\n",
    "for k in scores_dict.keys():\n",
    "    \n",
    "    df = scores_dict[k].copy()\n",
    "    \n",
    "    df.columns = df.columns.str.title()\n",
    "\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'O':\n",
    "            df[col] = df[col].str.title()           \n",
    "    \n",
    "    df.drop(df[df['Score'] < cutoff].index, inplace = True)\n",
    "    df = df.groupby('Score').head(3)\n",
    "    \n",
    "    sht.write(start_row + 1,\n",
    "              start_col + 5,\n",
    "              k.split(' ')[0].title(),\n",
    "              subtitle)\n",
    "                \n",
    "    for i in range(len(df.columns)):\n",
    "        \n",
    "        col_width = []        \n",
    "        \n",
    "        sht.write(start_row + 2,\n",
    "                 start_col + i + 5,\n",
    "                 df.columns[i],\n",
    "                 col_names)\n",
    "        \n",
    "        col_width.append(len(df.columns[i]))\n",
    "                \n",
    "        for j in range(len(df)):\n",
    "            \n",
    "            if df.columns[i] == 'Score':\n",
    "                fmt = percent\n",
    "            else:\n",
    "                fmt = dummy\n",
    "            \n",
    "            sht.write(start_row + 3 + j,\n",
    "                      start_col + i + 5,\n",
    "                      df.iloc[j,i],\n",
    "                      fmt)\n",
    "            \n",
    "            col_width.append(len(str(df.iloc[j,i])))\n",
    "            \n",
    "        sht.set_column(start_col + i,\n",
    "                       start_col + i + 5,\n",
    "                       np.max(col_width) + 1)\n",
    "        \n",
    "    start_col += len(df.columns) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if write_workbook == 'yes':\n",
    "    workbook.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

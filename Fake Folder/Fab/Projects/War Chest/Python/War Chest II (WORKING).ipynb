{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change this stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the last day from which you want historical data\n",
    "date_end = '2019-05-31'\n",
    "\n",
    "# do you want to run the script AND write the CSV? or just run the script?\n",
    "# the script will run regardless of what you choose here.\n",
    "# choose \"yes\" or \"no\"\n",
    "excel_write = 'no'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "tilde = os.path.expanduser('~')\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, tilde + '/Scripts/Fake Folder/Python Libraries')\n",
    "\n",
    "from jb_libraries import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Script Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "six_months_ago = str((pd.to_datetime(date_end) - pd.DateOffset(months = 5)).date())\n",
    "six_months_ago = six_months_ago[:7] + '-01'\n",
    "\n",
    "three_months_ago = str((pd.to_datetime(date_end) - pd.DateOffset(months = 2)).date())\n",
    "three_months_ago = three_months_ago[:7] + '-01'\n",
    "\n",
    "csv_path = tilde + '/Scripts/Fake Folder/Fab/Projects/War Chest/CSVs/'\n",
    "\n",
    "statuses = ['working', 'deprecated', 'pending', 'sample']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Fab and Kitting skus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skus_main = pd.read_sql(\n",
    "'''\n",
    "SELECT\n",
    "sku_date_modified,\n",
    "sku_id,\n",
    "k.part_id,\n",
    "pd.products_name,\n",
    "bom_type,\n",
    "p.products_dont_sell_alone,\n",
    "p.products_carrot_only,\n",
    "p.products_meta_type,\n",
    "p.products_coming_soon,\n",
    "p.discontinue_status,\n",
    "p.products_status,\n",
    "p.products_stripes,\n",
    "p.products_discontinued,\n",
    "sku_outsourced_assembly,\n",
    "k.sku_status\n",
    "FROM skus k\n",
    "LEFT JOIN products_description pd ON k.part_id = pd.part_id\n",
    "LEFT JOIN parts p ON k.part_id = p.part_id\n",
    "WHERE sku_status IN ''' + str(tuple(statuses)) + '''\n",
    "''', db)\n",
    "\n",
    "col_fix(skus_main)\n",
    "\n",
    "for col in skus_main.columns:\n",
    "    if skus_main[col].dtype == 'O':\n",
    "        skus_main[col] = skus_main[col].str.lower()\n",
    "        \n",
    "# turn \"resale - rapid prep\" and \"resale - no labor\" into just \"retail\"\n",
    "skus_main['bom type'] = skus_main['bom type'].str.split(' ', expand = True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flag outsourced, not, or both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = skus_main.groupby('part id')[['sku outsourced assembly']].sum()\n",
    "\n",
    "def outsourced(x):\n",
    "    \n",
    "    val = x['sku outsourced assembly']\n",
    "    \n",
    "    if val == 0:\n",
    "        ret = 'not outsourced'\n",
    "    elif val == 1:\n",
    "        ret = 'outsourced'\n",
    "    else:\n",
    "        ret = 'both'\n",
    "    return ret\n",
    "\n",
    "a['outsourced'] = a.apply(outsourced, axis = 1)\n",
    "\n",
    "print('proportions')\n",
    "vc = a['outsourced'].value_counts()\n",
    "print(vc/vc.sum())\n",
    "\n",
    "dict_ = dict(a['outsourced'])\n",
    "\n",
    "skus_main['outsourced'] = skus_main['part id'].map(dict_)\n",
    "\n",
    "skus_main.drop('sku outsourced assembly', 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shift data from sku level to part level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if there is more than one sku id per part id, then put them on one line\n",
    "df1 = pd.DataFrame(skus_main.groupby('part id')['sku id'].apply(lambda x: ', '.join(x.map(str))))\n",
    "skus_main['all sku ids'] = skus_main['part id'].map(dict(zip(df1.index.to_series(), df1['sku id'])))\n",
    "\n",
    "# if there is more than one bom type per part id, then put them on one line\n",
    "df2 = pd.DataFrame(skus_main.drop_duplicates(['part id','bom type']).groupby('part id')['bom type'].apply(lambda x: ', '.join(x.map(str))))\n",
    "skus_main['all bom types'] = skus_main['part id'].map(dict(zip(df2.index.to_series(), df2['bom type'])))\n",
    "\n",
    "skus_main.sort_values(['part id','sku date modified'], inplace = True)\n",
    "skus_main.drop_duplicates('part id', keep = 'last', inplace = True)\n",
    "\n",
    "skus_main.drop(['sku id','bom type'],1,inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify BOMs\n",
    "Some parts have multiple BOMs, for simplicity let's create a single BOM per part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in order of importance we'll do pnp, kitting, then resale\n",
    "\n",
    "def bom_fix(df):\n",
    "    if 'pnp' in df['all bom types']:\n",
    "        return 'pnp'\n",
    "    elif 'kitted' in df['all bom types']:\n",
    "        return 'kitted'\n",
    "    elif 'resale' in df['all bom types']:\n",
    "        return 'resale'\n",
    "    else:\n",
    "        return df['all bom types']\n",
    "    \n",
    "skus_main['all bom types'] = skus_main.apply(bom_fix, axis = 1)\n",
    "skus_main.rename(columns = {'all bom types':'bom type'}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all sales data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql(\n",
    "'''\n",
    "SELECT\n",
    "*\n",
    "FROM orders_status\n",
    "ORDER BY orders_status_id\n",
    "''', db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = dt.datetime.now()\n",
    "\n",
    "sales_super_main = pd.read_sql(\n",
    "'''\n",
    "SELECT\n",
    "DATE(o.date_purchased) AS date_purchased,\n",
    "DATE_FORMAT(o.date_purchased, '%Y-%m') AS year_and_month,\n",
    "op.orders_id,\n",
    "op.part_id,\n",
    "op.products_quantity AS qty_total, \n",
    "op.products_price\n",
    "FROM orders_products op\n",
    "JOIN orders o ON op.orders_id = o.orders_id\n",
    "# fraud - pending, fraud - confirmed, return, replaced defective, refunded defective, voided, fraud - void\n",
    "AND o.orders_status NOT IN (8,9,10,11,12,14,15)\n",
    "AND DATE(o.date_purchased) <= ' '''+ date_end +''' '\n",
    "WHERE (op.part_id IN '''+ str(tuple(skus_main['part id'].tolist())) +''' # get all valid skus\n",
    "OR op.part_id IN (SELECT part_id FROM products_to_stuff WHERE part_id > 0) # get all combos)\n",
    "AND op.part_id != 0\n",
    "''', db)\n",
    "\n",
    "col_fix(sales_super_main)\n",
    "\n",
    "# change to datetime\n",
    "sales_super_main['date purchased'] = pd.to_datetime(sales_super_main['date purchased'])\n",
    "\n",
    "d1 = sales_super_main['date purchased'].min().date()\n",
    "d2 = sales_super_main['date purchased'].max().date()\n",
    "\n",
    "print('data is from {} to {}\\n'.format(d1,d2))\n",
    "\n",
    "e = dt.datetime.now()\n",
    "print(e-s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_main = sales_super_main.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the combos that some parts are contained in\n",
    "We want to forecast the number of parts sold. If Part A is sold alone and there are x2 units in Combo Z, and if we sell x1 unit of Part A and x1 unit of Combo Z, then we are really selling x3 units of Part A. We need to make this adjustment to our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pts_main = pd.read_sql(\n",
    "'''\n",
    "SELECT\n",
    "part_id AS combo_part_id,\n",
    "contains_part_id AS part_id,\n",
    "pts_quantity\n",
    "FROM products_to_stuff\n",
    "# get combos only\n",
    "WHERE part_id > 0\n",
    "''', db)\n",
    "\n",
    "col_fix(pts_main)\n",
    "\n",
    "# flag \"parts\" or \"combos\"\n",
    "sales_main['part type'] = np.where(sales_main['part id'].isin(pts_main['combo part id'].tolist()), 'combo','part')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seperate out and remove combos from sales data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combos = sales_main[sales_main['part type'] == 'combo'].copy()\n",
    "\n",
    "combos.rename(columns = {'part id':'combo part id',\n",
    "                         'qty total':'combo qty total'}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Unpack\" combos into their parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge products_to_stuff, which contains the combo_part_id and all of the parts that that combo is made of\n",
    "combos = pd.merge(combos, pts_main, how = 'left', on = 'combo part id')\n",
    "\n",
    "# continuing the logic mentioned a few sections above:\n",
    "    # if a combo contains x2 units of Part A, and we sell x2 units of this combo, then we have really sold x4 units of Part A.\n",
    "    # make this adjustment here\n",
    "combos['qty total'] = combos['combo qty total'] * combos['pts quantity']\n",
    "\n",
    "# the price as it stands now in the combos dataframe is the price of the combo\n",
    "# but we just unpacked each combo into it's parts\n",
    "# so we need to overwrite the price of the combo with the price of each part\n",
    "price = pd.read_sql(\n",
    "'''\n",
    "SELECT\n",
    "part_id,\n",
    "products_price\n",
    "FROM parts\n",
    "''', db)\n",
    "\n",
    "col_fix(price)\n",
    "\n",
    "combos['products price'] = combos['part id'].map(dict(zip(price['part id'], price['products price'])))\n",
    "\n",
    "# check nulls\n",
    "n = combos[combos.isnull().any(1)]\n",
    "if n.empty:\n",
    "    print('no nulls')\n",
    "else:\n",
    "    print('you have %i nulls' % len(n))\n",
    "    display(n.head())\n",
    "    raise 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check out this example\n",
    "Valid as of 06/21/2019. Use the random OID below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oid = 2000914"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This order consists of x1 unique combo and x3 unique parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_main[sales_main['orders id'] == oid]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zoom into the combo, which is PN 1405. This combo conists of many parts. Take note below of the changes we made, specifically the products_price (which is now NOT the price of the combo, but the price of each part that this combo is made of), and the qty_total (which is combo_qty_total * pts_quantity).\n",
    "\n",
    "Going further with the qty_total, zoom into PN 617 below. Combo PN 1405 contains x2 units of PN 617. This person bought x100 units of Combo PN 1407, which means they bought 100 * 2 = 200 units of PN 617."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combos[combos['orders id'] == oid]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge combos dataframe with sales dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop combos from sales_main\n",
    "sales_main.drop(sales_main[sales_main['part type'] == 'combo'].index, inplace = True)\n",
    "\n",
    "# structure combos to match sales_main\n",
    "s1 = set(combos.columns)\n",
    "s2 = set(sales_main.columns)\n",
    "s3 = list(s1-s2)\n",
    "\n",
    "# drop any columns from combos which are not in sales_main\n",
    "combos.drop(s3, 1, inplace = True)\n",
    "\n",
    "# concat combos and sales_main\n",
    "sales_main = pd.concat([sales_main, combos], sort = False)\n",
    "sales_main.reset_index(inplace = True, drop = True)\n",
    "\n",
    "# drop this column\n",
    "sales_main.drop('part type', 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exclude some parts\n",
    "Because they possess at least one of the attributes below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('the sku statues we are considering are %s' % ', '.join(statuses))\n",
    "\n",
    "def exclude_flag(x):\n",
    "    if (x['products dont sell alone'] == 1\n",
    "        or x['products carrot only'] == 1\n",
    "        or x['products meta type'] == 'base product'\n",
    "        or x['products coming soon'] == 1\n",
    "        or x['discontinue status'] == 'discontinued'\n",
    "        or (x['products status'] == 1 and x['products stripes'] != 0)\n",
    "        or x['bom type'] == 'unspecified'\n",
    "        or x['sku status'] not in statuses):\n",
    "        res = 'yes'\n",
    "    else:\n",
    "        res = 'no'\n",
    "        \n",
    "    return res\n",
    "        \n",
    "skus_main['exclude'] = skus_main.apply(exclude_flag, axis = 1)\n",
    "\n",
    "# get parts to exclude based on conditions above\n",
    "ls1 = skus_main[skus_main['exclude'] == 'yes']['part id'].tolist()\n",
    "\n",
    "# any part that is in the sales data BUT NOT IN our skus data should be removed\n",
    "# recall that the skus data contains all valid parts\n",
    "ls2 = list(set(sales_main[~sales_main['part id'].isin(skus_main['part id'].tolist())]['part id']))\n",
    "\n",
    "# put them together\n",
    "ls3 = ls1 + ls2\n",
    "\n",
    "# get the parts we will exclude from the sales data\n",
    "x = list(set(sales_main[sales_main['part id'].isin(ls3)]['part id']))\n",
    "\n",
    "# exclude them\n",
    "sales_main.drop(sales_main[sales_main['part id'].isin(x)].index, inplace = True)\n",
    "sales_main.reset_index(inplace = True, drop = True)\n",
    "\n",
    "t = len(list(set(sales_main['part id'])))\n",
    "print('\\nwe will exclude {:,.0f} parts out of {:,.0f}, which is {:,.2f}% of the total count.'.format(len(x), t, len(x)/t*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map some sku data to sales data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = ['products name',\n",
    "      'outsourced',\n",
    "      'all sku ids',\n",
    "      'bom type']\n",
    "\n",
    "for col in ls:\n",
    "    sales_main[col] = sales_main['part id'].map(dict(zip(skus_main['part id'], skus_main[col])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check BOM proportions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc = sales_main['bom type'].value_counts()\n",
    "vc/vc.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine popularity tiers\n",
    "Use the qty sold and avg price, and use the latest three months worth of data. Note that we are using the data AFTER we have unpacked the combos. To determine the high, med, and low tiers we will use the qty sold only, but to determine the key tier we will use the revenue and qty sold of the UNPACKED COMBOS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the latest three months of data\n",
    "a = sales_main[sales_main['date purchased'] >= three_months_ago].copy()\n",
    "\n",
    "# map bom types\n",
    "a['bom type'] = a['part id'].map(dict(zip(sales_main['part id'], sales_main['bom type'])))\n",
    "\n",
    "# get revenue\n",
    "a['revenue'] = a['qty total'] * a['products price']\n",
    "\n",
    "# get totals by bom and part_id\n",
    "b = a.groupby(['bom type','part id'])[['qty total','revenue']].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine popularity tiers for each BOM\n",
    "alpha = 0.15\n",
    "popularity_tiers = pd.DataFrame()\n",
    "\n",
    "for bom in set(b.index.get_level_values(0)):\n",
    "    \n",
    "    # isolate the BOM\n",
    "    df = b[b.index.get_level_values(0) == bom].copy()\n",
    "    df.reset_index(inplace = True)\n",
    "    \n",
    "    # any part with fewer than three months of data gets a tier of \"not enough data\"\n",
    "    g = a[a['bom type'] == bom].groupby('part id')[['year and month']].nunique()\n",
    "    p = g[(g < 3).any(1)].index.tolist()\n",
    "    not_enough_data = df[df['part id'].isin(p)].copy()\n",
    "    \n",
    "    # remove these \"not enough data parts\" from data\n",
    "    df.drop(df[df['part id'].isin(not_enough_data)].index, inplace = True)\n",
    "    df.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "    # get top revenue and top qty\n",
    "    q1 = df['qty total'].quantile(1-alpha)\n",
    "    q2 = df['revenue'].quantile(1-alpha)\n",
    "    \n",
    "    # get key parts which are parts we sell the most of AND make the most money on\n",
    "    key = df[(df['qty total'] >= q1) & (df['revenue'] >= q2)].copy()\n",
    "    \n",
    "    # remove the key parts\n",
    "    df.drop(df[df['part id'].isin(key['part id'].tolist())].index, inplace = True)\n",
    "    df.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "    # now get the high (above 75%), med (between 50% and 75%), and low (below 50%)\n",
    "    ls1 = [0.75, 0.50]\n",
    "    ls2 = []\n",
    "    for p in ls1:\n",
    "        val = df['qty total'].quantile(p)\n",
    "        ls2.append(val)\n",
    "        \n",
    "    high = df[df['qty total'] >= ls2[0]].copy()\n",
    "    med = df[np.logical_and(df['qty total'] < ls2[0], df['qty total'] >= ls2[1])].copy()\n",
    "    low = df[df['qty total'] < ls2[1]].copy()\n",
    "    \n",
    "    # store the tiers\n",
    "    tiers = {'key':key,\n",
    "             'high':high,\n",
    "             'med':med,\n",
    "             'low':low,\n",
    "             'not enough data':not_enough_data}\n",
    "    \n",
    "    # add this column to each tier\n",
    "    for k in tiers.keys():\n",
    "        tiers[k]['popularity tier'] = k\n",
    "        popularity_tiers = pd.concat([popularity_tiers, tiers[k]], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge popularity tier data with sales data\n",
    "sales_main = pd.merge(sales_main,\n",
    "                      popularity_tiers[['bom type','part id','popularity tier']],\n",
    "                      on = ['bom type','part id'],\n",
    "                      how = 'left')\n",
    "\n",
    "# get the popularity tier nulls\n",
    "# a true popularity tier null means that we do not have sales data within the last three months \n",
    "x = list(set(sales_main[sales_main['popularity tier'].isnull()]['part id']))\n",
    "\n",
    "# check to make sure these nulls really mean that we don't have data within the last three months\n",
    "# nulls which should not be nulls will appear here\n",
    "df = sales_main[sales_main['part id'].isin(x)].groupby('part id')[['date purchased']].max()\n",
    "\n",
    "# if all nulls are really a result of us not having data within the last three months, fill these nulls with \"low\"\n",
    "# if we don't have sales data from the last three months, then no one bought part in the last three months, and this warrants a low popularity tier\n",
    "if np.sum(df['date purchased'] >= three_months_ago) == 0:\n",
    "    sales_main['popularity tier'].fillna('low', inplace = True)\n",
    "else:\n",
    "    raise ValueError('you have popularity tier nulls which should not be nulls.\\ntfix this before moving on.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('count of unique parts within each BOM and popularity tier')\n",
    "check = sales_main.groupby(['bom type','popularity tier'])[['part id']].nunique().unstack(1)\n",
    "check.columns = check.columns.droplevel(0)\n",
    "check = check[['key','high','med','low','not enough data']]\n",
    "check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run part forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the forecast library\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# structure the data for forecasting\n",
    "df = sales_main.groupby(['bom type','popularity tier','part id','year and month'])[['qty total']].sum().unstack(3).fillna(0)\n",
    "\n",
    "# determine train (80%) and test (20%) sets\n",
    "cut_off = int(np.round(len(df.columns) * 0.80))\n",
    "\n",
    "train = df.iloc[:, :cut_off]\n",
    "test = df.iloc[:, cut_off:]\n",
    "\n",
    "# check\n",
    "if len(train.columns) + len(test.columns) != len(df.columns):\n",
    "    raise ValueError('the sum of the train and test columns do not match the number of columns in the original dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auto_arima for python\n",
    "#https://github.com/tgsmith61591/pmdarima/blob/master/examples/quick_start_example.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# determine parameters for ARIMA models\n",
    "ls1 = [0,1,2,3]\n",
    "ls2 = [0,1]\n",
    "ls3 = []\n",
    "ls4 = []\n",
    "for i in ls1:\n",
    "    for j in ls2:\n",
    "        ls3.append(tuple([i,j,i]))\n",
    "        ls4.append(tuple([i,j,i,12]))\n",
    "\n",
    "params = [(x,y) for x in ls3 for y in ls4]\n",
    "\n",
    "# for each part, run the forecasts\n",
    "#for i in range(len(df)):\n",
    "for i in range(5):\n",
    "    counter = 0\n",
    "    values = train.iloc[i,:].values\n",
    "    for p in params:\n",
    "        try:\n",
    "            model = sm.tsa.statespace.SARIMAX(values,\n",
    "                                              order = p[0],\n",
    "                                              seasonal_order = p[1])\n",
    "            fit = model.fit()\n",
    "            counter += 1\n",
    "        except:\n",
    "            pass\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data for forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_forecast = pd.DataFrame()\n",
    "\n",
    "for pn in list(set(sales_clean['umbrella part id'])):\n",
    "    \n",
    "    # get qty per month\n",
    "    df1 = sales_clean[sales_clean['umbrella part id'] == pn].groupby('year and month')[['mod qty total']].sum()\n",
    "\n",
    "    # fill in months where no one bought anything\n",
    "    # make the final year and month the date_end of this script\n",
    "    d1 = df1.index.min()\n",
    "    d2 = date_end[:7]\n",
    "    \n",
    "    dates = pd.date_range(d1, d2, freq = 'MS')\n",
    "    \n",
    "    # make it so that every PN has at least 6 months of data\n",
    "    n = 6\n",
    "    l = len(dates)\n",
    "    if l < n:\n",
    "        new_d1 = pd.to_datetime(d1) - pd.DateOffset(months = (n-l))\n",
    "        new_d1 = str(new_d1)[:7]\n",
    "        dates = pd.date_range(new_d1, d2, freq = 'MS')\n",
    "    \n",
    "    dates = pd.DataFrame(dates, columns = ['year and month']).set_index('year and month')\n",
    "    # join them\n",
    "    df2 = dates.join(df1).fillna(0) \n",
    "    df2['umbrella part id'] = pn\n",
    "    df2.reset_index(inplace = True)\n",
    "    \n",
    "    # create one big dataframe, for the forecast\n",
    "    for_forecast = for_forecast.append(df2, ignore_index = True)\n",
    "\n",
    "# map popularity tier and bom\n",
    "for col in ['bom type','popularity tier']:\n",
    "    for_forecast[col] = for_forecast['umbrella part id'].map(dict(zip(sales_clean['umbrella part id'], sales_clean[col])))\n",
    "    \n",
    "# rename these for a layman to understand\n",
    "for_forecast.rename(columns = {'umbrella part id':'part id',\n",
    "                              'mod qty total':'qty total'}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get sku ids and part names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.read_sql(\n",
    "'''\n",
    "SELECT\n",
    "k.sku_id,\n",
    "k.part_id,\n",
    "pd.products_name\n",
    "FROM skus k\n",
    "JOIN products_description pd ON k.part_id = pd.part_id\n",
    "WHERE k.sku_status IN ''' + str(tuple(statuses)) + '''\n",
    "AND k.part_id IN '''+ str(tuple(for_forecast['part id'].tolist())) +'''\n",
    "''', db)\n",
    "\n",
    "col_fix(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['sku id','products name']:\n",
    "    for_forecast[col] = for_forecast['part id'].map(dict(zip(s['part id'], s[col])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.sum(for_forecast.isnull().any(1))\n",
    "if n == 0:\n",
    "    pass\n",
    "else:\n",
    "    raise 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check count of data points per PN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = np.sum(for_forecast.groupby('part id')[['year and month']].count() < n).values[0]\n",
    "if c == 0:\n",
    "    pass\n",
    "else:\n",
    "    raise 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check unique PN count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = len(set(for_forecast['part id']))\n",
    "v2 = len(set(sales_clean['umbrella part id']))\n",
    "if v1 == v2:\n",
    "    print('match')\n",
    "    print('the total unique umbrella part id count is {}'.format(v1))\n",
    "else:\n",
    "    raise 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check quantities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = for_forecast['qty total'].sum()\n",
    "q2 = sales_clean['mod qty total'].sum()\n",
    "\n",
    "if np.abs(q1 - q2) < 0.01:\n",
    "    pass\n",
    "else:\n",
    "    raise 0\n",
    "    \n",
    "# fill zeros with 0.1, to ease forecasting\n",
    "for_forecast['qty total'] = np.where(for_forecast['qty total'] == 0, 0.1, for_forecast['qty total'])        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map sku and part data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = skus_main.columns.tolist()\n",
    "for x in ['part id','products name']:\n",
    "    cols.remove(x)\n",
    "\n",
    "for c in cols:\n",
    "    for_forecast[c] = for_forecast['part id'].map(dict(zip(skus_main['part id'], skus_main[c])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To Excel, for R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if excel_write == 'yes':\n",
    "    \n",
    "    writer = pd.ExcelWriter(csv_path + 'War Chest Part sales for R.xlsx', engine = 'xlsxwriter')\n",
    "    for_forecast.to_excel(writer, 'data', index = False)\n",
    "    writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
